# -*- coding: utf-8 -*-
"""
Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1I1bpOj-36lMYcMh857eeYaYh959Iu5wk
"""

# Commented out IPython magic to ensure Python compatibility.
import pymc3 as pm

def main():
    
    with pm.Model() as model:
      # Using a strong prior. Meaning the mean is towards zero than towards 1
      prior = pm.Beta('prior', 0.5, 3)

      output = pm.Binomial('output', n = 100, observed = 50, p = prior)

      step = pm.Metropolis()
      trace = pm.sample(1000, step = step)
      pm.traceplot(trace)

    pm.plot_posterior(trace,
                      figsize=(5, 5),
                      kde_plot = True,
                      rope = [0.45, 0.55]) # Rope is an interval that you define
                                           # This is a value you eppect. You can check
                                           # If ROPE fall on HPD or not. If it falls, it means
                                           # our value is within HPD and may be increasing sample
                                           # size would make our mean estimate better.


    # gelman rubin
    pm.gelman_rubin(trace)

    # forestplot
    pm.forestplot(trace, varnames = ['prior'])

    # summary [look at mc error here. This is the std error, should be low]
    pm.df_summary(trace)

    #autocorrelation
    pm.autocorrplot(trace)

    # effective size
    pm.effective_n(trace)['prior']


if '__name__' == '__main__':
    main()

    
'''
You should play with the alpha and beta parameters of
beta distribution and see how the HPD changes.

You should also play with n and observed, meaning increase the
number of data points.

When you use a strong prior and little data, you will see
the posterior resemble the shape of the prior. Whereas for the same prior,
if you increase the number of datapoints, you will see influence of data more strong




Bernoulli distribtion is for a single experiment with yes or no answer.
Binomial distribution is a series of bernouli distribution.
Binomial distribution with n = 1 is a bernouli distribution.

Therefore, in the above program if you replace n = 1, this becomes a binomial distribution
output = pm.Binomial('output',  n = 1, p = prior, observed = [0, 1])



Autocorrelation [Bayesian data analysis- Oswaldo martin]
--------
This is a measure of how effectively the algorithm sampled the parameter space.
If it is not efficient, the samples (lets say every 100 points) will be correlated.
But we seek to sample points that are least autocorrelated.
1. An ideal sample will lack autocorrelation, that is, a value at one
point should be independent of the values at other points.

2. The plot shows the average correlation of sample values
compared to successive points (up to 100 points).
Ideally we should see no autocorrelation, in practice;
we seek samples that quickly drop to low values of
autocorrelation. The more autocorrelated a parameter
is, the larger the number of samples we will need to
obtain a given precision; that is, autocorrelation has
the detrimental effect of lowering the effective number of samples. 

Effective size
-----
Once you run a bayesian update and observe there is high autocorrelation.
looking for effective sample size will tell you, how many samples you would
need to get samples without autocorrelation

A sample with autocorrelation has less information than a
sample of the same size without autocorrelation.
Hence, given a sample of a certain size with a certain
degree of autocorrelation we could try to estimate what
will be the size of the sample with the same information
without autocorrelation. That number will be the effective
size of the sample. Ideally both quantities should be the
same; the closer the two numbers the more efficient our
sampling. The effective size of sample could serve us as a guide.
If we want to estimate mean values of a distribution we will
need an effective sample of at least 100 samples; if we want
to estimate quantities that depend on the tails of distribution,
such as the limits of credible intervals, we will need an
effective size of 1000 to 10000 samples.


'''
