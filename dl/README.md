With the realization that learning in chunks is better than learning to use stuff made me to make use of resources that I already saved. I plan to create a jupyter notebook where I plan to break down the concept and learn to implement things from scratch one at a time. Some of the best resources I found today are a masters thesis where the project was on implementing an activation function for bayesian neural network. All his work are saved in folder 
C:\RahulGit\Bayesian_ml_dl_workout_area\Master-Thesis-BayesianCNN-master

Chunk 1: https://github.com/wiseodd/hipsternet (Guys blog: https://wiseodd.github.io/techblog/2016/12/21/forward-reverse-kl/)
Chunk 2: https://ericmjl.github.io/essays-on-data-science/machine-learning/computational-bayesian-stats/


I found this blog post "learning to learn" really interesting. He talks about how he implemented all deep learning functions without using any libraries.

https://ericmjl.github.io/essays-on-data-science/miscellaneous/learning-to-learn/

He also has implementation of functions for bayesian methods. His deep learning and machine learning implementations can be accessed from here:
https://ericmjl.github.io/essays-on-data-science/machine-learning/reimplementing-models/
https://ericmjl.github.io/essays-on-data-science/machine-learning/computational-bayesian-stats/

Variational inference code can be accessed from here: https://github.com/HIPS/autograd

The guy who wrote the masters thesis has several github code. I tired one of his code here: https://colab.research.google.com/drive/1uVO7csBL15VB0YT7z-3obJSGgrRr47a9#scrollTo=7NKbNddHeDpa

I also found these statistical rethining repositories in pyro and pytorch interesting to compare and learn better: 
https://fehiepsi.github.io/rethinking-pyro/02-small-worlds-and-large-worlds.html
https://fehiepsi.github.io/rethinking-numpyro/02-small-worlds-and-large-worlds.html

This kaggle competition code using pytorch is good to learn myself.
https://github.com/fehiepsi/pytorch-notebooks/blob/master/executable/Fish.ipynb


This is an amazing resource for learning bayesian in tiny chunks
C:\RahulGit\Bayesian_ml_dl_workout_area\OTHER RESOURCES\probabilisticprogrammingprimer-master


Algorithms to learn to implement
https://wiseodd.github.io/techblog/2015/10/17/metropolis-hastings/


Convolutional layer
https://wiseodd.github.io/techblog/2016/07/16/convnet-conv-layer/

Maxpool layer
https://wiseodd.github.io/techblog/2016/07/18/convnet-maxpool-layer/

Implementing dropout
https://wiseodd.github.io/techblog/2016/06/25/dropout/

sampling
https://wiseodd.github.io/techblog/2017/09/07/lda-gibbs/
gradient descent: https://wiseodd.github.io/techblog/2018/03/14/natural-gradient/
